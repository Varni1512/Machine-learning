{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7ff0f0-7e41-4d49-82db-cec3c0ba1f1e",
   "metadata": {},
   "source": [
    "### Pandas Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31196b-466b-49b2-9bdd-6193d9df1c74",
   "metadata": {},
   "source": [
    "Pandas is a powerful and open-source Python library. The Pandas library is used for data manipulation and analysis. Pandas consist of data structures and functions to perform efficient operations on data.\n",
    "\n",
    "Pandas is well-suited for working with tabular data, such as spreadsheets or SQL tables.\n",
    " - Installing & Importing Pandas\n",
    " - Pandas Data Structures: Series and DataFrame\n",
    " - Loading and Saving Data\n",
    " - Exploring Data\n",
    " - Indexing and Selecting Data\n",
    " - Data Cleaning\n",
    " - Data Transformation\n",
    " - Filtering and Sorting\n",
    " - Grouping and Aggregating\n",
    " - Merging and Joining\n",
    " - Handling Dates and Times\n",
    " - Pivot Tables\n",
    " - Working with Large Datasets\n",
    " - Advanced Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15f8ad-71b2-4369-b583-cffec91d1cd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Installing & Importing Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a21926c-4979-41b7-916f-6d77a133e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f34c81-4262-41c1-87b9-635be79d10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c749f0-fae3-4ffd-809d-8b8efcad1302",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pandas Data Structures: Series and DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c6967-86f2-4944-8472-fc6d145ad756",
   "metadata": {},
   "source": [
    " - Series: A 1D labeled array, like a single column.\n",
    " - DataFrame: A 2D labeled structure, like a table with rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5a2e3-e199-40a0-8517-3a88ec6aabe8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Pandas Series (1D Labeled Array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465496d4-6c06-44f1-92f6-184518961d78",
   "metadata": {},
   "source": [
    "A Series is a one-dimensional labeled array that can hold any data type (integers, floats, strings, Python objects, etc.). It is similar to a column in a spreadsheet or a list with labels (index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bcfc89-72e6-4389-8fdb-28a09b7003f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Series:\n",
      "A    10\n",
      "B    20\n",
      "C    30\n",
      "D    40\n",
      "dtype: int64\n",
      "\n",
      "Accessing Element at Index 'B': 20\n",
      "\n",
      "Filtering Elements > 20:\n",
      "C    30\n",
      "D    40\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a Series from a List\n",
    "data = [10, 20, 30, 40]\n",
    "series = pd.Series(data, index=['A', 'B', 'C', 'D'])\n",
    "\n",
    "# Display Series\n",
    "print(\"Pandas Series:\")\n",
    "print(series)\n",
    "\n",
    "# Accessing elements\n",
    "print(\"\\nAccessing Element at Index 'B':\", series['B'])\n",
    "\n",
    "# Filtering elements\n",
    "print(\"\\nFiltering Elements > 20:\")\n",
    "print(series[series > 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfceb5d-2dd6-4df0-9451-7ef525552576",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Pandas DataFrame (2D Table-like Structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67243bd6-f1e6-47bb-a882-ded157f47b87",
   "metadata": {},
   "source": [
    "A DataFrame is a two-dimensional labeled data structure similar to an Excel spreadsheet or SQL table.\n",
    " - It consists of rows and columns.\n",
    " - Each column is a Series.\n",
    " - It can hold different data types (integers, floats, strings, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3895ba9-3571-48ed-a8e1-49ddd48dc86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:\n",
      "      Name  Age  Salary\n",
      "0    Alice   25   50000\n",
      "1      Bob   30   60000\n",
      "2  Charlie   35   70000\n",
      "\n",
      "Selecting 'Name' Column:\n",
      "0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "Name: Name, dtype: object\n",
      "\n",
      "DataFrame After Adding 'Bonus' Column:\n",
      "      Name  Age  Salary   Bonus\n",
      "0    Alice   25   50000  5000.0\n",
      "1      Bob   30   60000  6000.0\n",
      "2  Charlie   35   70000  7000.0\n",
      "\n",
      "Filtered Data (Age > 25):\n",
      "      Name  Age  Salary   Bonus\n",
      "1      Bob   30   60000  6000.0\n",
      "2  Charlie   35   70000  7000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame from a Dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Selecting a column\n",
    "print(\"\\nSelecting 'Name' Column:\")\n",
    "print(df['Name'])\n",
    "\n",
    "# Adding a new column\n",
    "df['Bonus'] = df['Salary'] * 0.10\n",
    "print(\"\\nDataFrame After Adding 'Bonus' Column:\")\n",
    "print(df)\n",
    "\n",
    "# Filtering rows\n",
    "filtered_df = df[df['Age'] > 25]\n",
    "print(\"\\nFiltered Data (Age > 25):\")\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6493d-bddc-409b-b512-70472c32b06a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  Loading and Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f8523c-bda8-4fde-961a-f80a92993146",
   "metadata": {},
   "source": [
    "When working with machine learning and data analysis, it is essential to load and save data efficiently. Pandas provides various functions to handle different file formats like CSV, Excel, JSON, SQL, Pickle, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0a391-5508-4f67-bcc3-90317d5d823f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec8fe97-c634-4308-8ab1-e653b7db09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Data:\n",
      "   ID     Name  Age         City\n",
      "0   1    Alice   25     New York\n",
      "1   2      Bob   30  Los Angeles\n",
      "2   3  Charlie   28      Chicago\n",
      "3   4    David   35      Houston\n",
      "\n",
      "Excel File Data:\n",
      "   ID     Name  Age         City\n",
      "0   1    Alice   25     New York\n",
      "1   2      Bob   30  Los Angeles\n",
      "2   3  Charlie   28      Chicago\n",
      "3   4    David   35      Houston\n",
      "\n",
      "JSON File Data:\n",
      "   ID     Name  Age         City\n",
      "0   1    Alice   25     New York\n",
      "1   2      Bob   30  Los Angeles\n",
      "2   3  Charlie   28      Chicago\n",
      "3   4    David   35      Houston\n",
      "\n",
      "SQL Database Data:\n",
      "   ID           Name  Age Department  Salary\n",
      "0   1       John Doe   29         HR   50000\n",
      "1   2     Jane Smith   34    Finance   65000\n",
      "2   3  Emily Johnson   28         IT   70000\n",
      "3   4  Michael Brown   40  Marketing   62000\n",
      "4   5   Chris Wilson   35      Sales   58000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Loading CSV File\n",
    "df_csv = pd.read_csv('data.csv')\n",
    "print(\"CSV File Data:\")\n",
    "print(df_csv.head())\n",
    "\n",
    "# Loading Excel File\n",
    "df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
    "print(\"\\nExcel File Data:\")\n",
    "print(df_excel.head())\n",
    "\n",
    "# Loading JSON File\n",
    "df_json = pd.read_json('data.json')\n",
    "print(\"\\nJSON File Data:\")\n",
    "print(df_json.head())\n",
    "\n",
    "# Loading Data from SQL Database\n",
    "conn = sqlite3.connect('database.db')\n",
    "df_sql = pd.read_sql('SELECT * FROM employees', conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nSQL Database Data:\")\n",
    "print(df_sql.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c69ea-9c5c-4aea-853f-0044f79411af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Code for Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a7de2c-b332-4a07-b328-ae007f5b41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to output.csv\n",
      "Data saved to output.xlsx\n",
      "Data saved to output.json\n",
      "Data saved to SQL database\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Saving as CSV\n",
    "df.to_csv('output.csv', index=False)\n",
    "print(\"Data saved to output.csv\")\n",
    "\n",
    "# Saving as Excel\n",
    "df.to_excel('output.xlsx', index=False, sheet_name='Results')\n",
    "print(\"Data saved to output.xlsx\")\n",
    "\n",
    "# Saving as JSON\n",
    "df.to_json('output.json', orient='records', indent=4)\n",
    "print(\"Data saved to output.json\")\n",
    "\n",
    "# Saving to SQL Database\n",
    "conn = sqlite3.connect('database.db')\n",
    "df.to_sql('employees', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "print(\"Data saved to SQL database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18b7e0-175f-4b48-8a00-160f4d9408a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898340c5-a86d-40cd-882c-2093d0fa613b",
   "metadata": {},
   "source": [
    "After loading data into Pandas, the next step is to explore, understand, and analyze the dataset.This helps in preprocessing and feature engineering for Machine Learning (ML) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019e02af-adcd-48ae-865a-df01859fe608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   ID            10 non-null     int64 \n",
      " 1   Name          10 non-null     object\n",
      " 2   Age           10 non-null     int64 \n",
      " 3   Salary        10 non-null     int64 \n",
      " 4   Category      10 non-null     object\n",
      " 5   Joining_Date  10 non-null     object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 612.0+ bytes\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "   ID     Name  Age  Salary Category Joining_Date\n",
      "0   1    Alice   25   50000        A   2020-01-15\n",
      "1   2      Bob   30   60000        B   2019-06-20\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "3   4    David   28   48000        C   2021-07-25\n",
      "4   5      Eva   22   52000        B   2020-09-30\n",
      "\n",
      "Summary Statistics:\n",
      "             ID        Age        Salary\n",
      "count  10.00000  10.000000     10.000000\n",
      "mean    5.50000  30.400000  59700.000000\n",
      "std     3.02765   5.440588   9068.259664\n",
      "min     1.00000  22.000000  48000.000000\n",
      "25%     3.25000  27.250000  52500.000000\n",
      "50%     5.50000  29.500000  59000.000000\n",
      "75%     7.75000  34.250000  66500.000000\n",
      "max    10.00000  40.000000  75000.000000\n",
      "\n",
      "Missing Values Count:\n",
      "ID              0\n",
      "Name            0\n",
      "Age             0\n",
      "Salary          0\n",
      "Category        0\n",
      "Joining_Date    0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values Count:\n",
      "ID              10\n",
      "Name            10\n",
      "Age             10\n",
      "Salary          10\n",
      "Category         3\n",
      "Joining_Date    10\n",
      "dtype: int64\n",
      "\n",
      "Column Data Types:\n",
      "ID               int64\n",
      "Name            object\n",
      "Age              int64\n",
      "Salary           int64\n",
      "Category        object\n",
      "Joining_Date    object\n",
      "dtype: object\n",
      "\n",
      "Category Counts:\n",
      " Category\n",
      "A    4\n",
      "B    3\n",
      "C    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Correlation Matrix:\n",
      "               ID       Age    Salary\n",
      "ID      1.000000  0.263070  0.269122\n",
      "Age     0.263070  1.000000  0.899038\n",
      "Salary  0.269122  0.899038  1.000000\n",
      "\n",
      "Sorted by Salary (Top 5):\n",
      "   ID     Name  Age  Salary Category Joining_Date\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "7   8    Helen   29   62000        B   2021-10-22\n",
      "1   2      Bob   30   60000        B   2019-06-20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# 1. Display basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# 2. Display the first 5 rows\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Summary statistics for numerical columns\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 4. Check for missing values in the dataset\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 5. Display unique value counts for each column\n",
    "print(\"\\nUnique Values Count:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# 6. Show data types of each column\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 7. Value counts of a categorical column ('Category')\n",
    "if 'Category' in df.columns:\n",
    "    print(\"\\nCategory Counts:\\n\", df['Category'].value_counts())\n",
    "\n",
    "# 8. Correlation matrix (Only numerical columns)\n",
    "numeric_df = df.select_dtypes(include=['number'])  # Selecting only numerical columns\n",
    "print(\"\\nCorrelation Matrix:\\n\", numeric_df.corr())\n",
    "\n",
    "# 9. Sorting data by Salary in descending order (Change 'Salary' to another numeric column if needed)\n",
    "if 'Salary' in df.columns:\n",
    "    print(\"\\nSorted by Salary (Top 5):\")\n",
    "    print(df.sort_values(by='Salary', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb31e91-e919-4a82-b653-f727b7f09c9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Indexing and Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fff506-520c-4399-a8cf-adaaf0d7ff06",
   "metadata": {},
   "source": [
    "Indexing and selecting data in Pandas allows us to retrieve specific rows, columns, or elements from a DataFrame or Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbd264f-f0f7-4869-9d33-17b091c6bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting 'Name' column:\n",
      " 0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "3      David\n",
      "4        Eva\n",
      "5      Frank\n",
      "6      Grace\n",
      "7      Helen\n",
      "8        Ian\n",
      "9       Jack\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Selecting 'Name' and 'Age' columns:\n",
      "       Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "3    David   28\n",
      "4      Eva   22\n",
      "5    Frank   40\n",
      "6    Grace   32\n",
      "7    Helen   29\n",
      "8      Ian   27\n",
      "9     Jack   36\n",
      "\n",
      "Row at index 2 using loc:\n",
      " ID                       3\n",
      "Name               Charlie\n",
      "Age                     35\n",
      "Salary               70000\n",
      "Category                 A\n",
      "Joining_Date    2018-03-12\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Rows from index 2 to 5 using loc:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "3   4    David   28   48000        C   2021-07-25\n",
      "4   5      Eva   22   52000        B   2020-09-30\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "\n",
      "Selecting 'Name' and 'Salary' columns using loc:\n",
      "       Name  Salary\n",
      "0    Alice   50000\n",
      "1      Bob   60000\n",
      "2  Charlie   70000\n",
      "3    David   48000\n",
      "4      Eva   52000\n",
      "5    Frank   75000\n",
      "6    Grace   58000\n",
      "7    Helen   62000\n",
      "8      Ian   54000\n",
      "9     Jack   68000\n",
      "\n",
      "Row at position 3 using iloc:\n",
      " ID                       4\n",
      "Name                 David\n",
      "Age                     28\n",
      "Salary               48000\n",
      "Category                 C\n",
      "Joining_Date    2021-07-25\n",
      "Name: 3, dtype: object\n",
      "\n",
      "Rows from position 2 to 5 using iloc:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "3   4    David   28   48000        C   2021-07-25\n",
      "4   5      Eva   22   52000        B   2020-09-30\n",
      "\n",
      "Selecting first and third columns using iloc:\n",
      "    ID  Age\n",
      "0   1   25\n",
      "1   2   30\n",
      "2   3   35\n",
      "3   4   28\n",
      "4   5   22\n",
      "5   6   40\n",
      "6   7   32\n",
      "7   8   29\n",
      "8   9   27\n",
      "9  10   36\n",
      "\n",
      "People older than 30:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "\n",
      "People in Category 'A':\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "0   1    Alice   25   50000        A   2020-01-15\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "\n",
      "People with Age > 25 and Salary > 50000:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "1   2      Bob   30   60000        B   2019-06-20\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "7   8    Helen   29   62000        B   2021-10-22\n",
      "8   9      Ian   27   54000        C   2020-11-05\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "\n",
      "Selecting rows 2 to 5 with 'Name' and 'Salary':\n",
      "       Name  Salary\n",
      "2  Charlie   70000\n",
      "3    David   48000\n",
      "4      Eva   52000\n",
      "5    Frank   75000\n",
      "\n",
      "Selecting specific rows and first 3 columns:\n",
      "    ID     Name  Age\n",
      "1   2      Bob   30\n",
      "2   3  Charlie   35\n",
      "3   4    David   28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# 1. Selecting a single column\n",
    "print(\"\\nSelecting 'Name' column:\\n\", df[\"Name\"])\n",
    "\n",
    "# 2. Selecting multiple columns\n",
    "print(\"\\nSelecting 'Name' and 'Age' columns:\\n\", df[[\"Name\", \"Age\"]])\n",
    "\n",
    "# 3. Selecting rows using loc (label-based)\n",
    "print(\"\\nRow at index 2 using loc:\\n\", df.loc[2])\n",
    "print(\"\\nRows from index 2 to 5 using loc:\\n\", df.loc[2:5])\n",
    "print(\"\\nSelecting 'Name' and 'Salary' columns using loc:\\n\", df.loc[:, [\"Name\", \"Salary\"]])\n",
    "\n",
    "# 4. Selecting rows using iloc (position-based)\n",
    "print(\"\\nRow at position 3 using iloc:\\n\", df.iloc[3])\n",
    "print(\"\\nRows from position 2 to 5 using iloc:\\n\", df.iloc[2:5])\n",
    "print(\"\\nSelecting first and third columns using iloc:\\n\", df.iloc[:, [0, 2]])\n",
    "\n",
    "# 5. Filtering data based on conditions\n",
    "print(\"\\nPeople older than 30:\\n\", df[df[\"Age\"] > 30])\n",
    "print(\"\\nPeople in Category 'A':\\n\", df[df[\"Category\"] == \"A\"])\n",
    "print(\"\\nPeople with Age > 25 and Salary > 50000:\\n\", df[(df[\"Age\"] > 25) & (df[\"Salary\"] > 50000)])\n",
    "\n",
    "# 6. Selecting specific rows and columns\n",
    "print(\"\\nSelecting rows 2 to 5 with 'Name' and 'Salary':\\n\", df.loc[2:5, [\"Name\", \"Salary\"]])\n",
    "print(\"\\nSelecting specific rows and first 3 columns:\\n\", df.iloc[1:4, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d3c69-af1f-4872-8a1a-0e7eac3a3356",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d88d4e5-3413-46ba-ac0c-827ddde11675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      " ID              0\n",
      "Name            0\n",
      "Age             0\n",
      "Salary          0\n",
      "Category        0\n",
      "Joining_Date    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows before removal: 0\n",
      "Warning: 'Date' column not found in the dataset!\n",
      "\n",
      "Cleaned Data:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "0   1    Alice   25   50000        a   2020-01-15\n",
      "1   2      Bob   30   60000        b   2019-06-20\n",
      "2   3  Charlie   35   70000        a   2018-03-12\n",
      "3   4    David   28   48000        c   2021-07-25\n",
      "4   5      Eva   22   52000        b   2020-09-30\n",
      "\n",
      "Cleaned data saved as 'cleaned_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# 1. Handling Missing Data\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "df.fillna({\"Age\": df[\"Age\"].mean(), \"Salary\": df[\"Salary\"].median()}, inplace=True)\n",
    "\n",
    "# 2. Removing Duplicates\n",
    "print(\"\\nDuplicate rows before removal:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 3. Standardizing Text Data\n",
    "df[\"Category\"] = df[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "# 4. Converting Data Types\n",
    "df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
    "\n",
    "# Check if \"Date\" exists before converting\n",
    "if \"Date\" in df.columns:\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "else:\n",
    "    print(\"Warning: 'Date' column not found in the dataset!\")\n",
    "\n",
    "# 5. Handling Outliers (IQR Method)\n",
    "if \"Salary\" in df.columns:\n",
    "    Q1 = df[\"Salary\"].quantile(0.25)\n",
    "    Q3 = df[\"Salary\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[(df[\"Salary\"] >= Q1 - 1.5 * IQR) & (df[\"Salary\"] <= Q3 + 1.5 * IQR)]\n",
    "else:\n",
    "    print(\"Warning: 'Salary' column not found in the dataset!\")\n",
    "\n",
    "print(\"\\nCleaned Data:\\n\", df.head())\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "print(\"\\nCleaned data saved as 'cleaned_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2aa778-12ef-4170-b3de-7bee384e2570",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf249cb-412e-4c7e-9d9a-4cbc48534962",
   "metadata": {},
   "source": [
    "Data transformation involves modifying the structure, format, or values of data to make it suitable for analysis and modeling. The key transformations include:\n",
    "\n",
    " - Scaling & Normalization – Adjusting numerical values to a common scale\n",
    " - Encoding Categorical Variables – Converting categorical data into numerical form\n",
    " - Feature Engineering – Creating new features from existing ones\n",
    " - Aggregation & Grouping – Summarizing data at different levels\n",
    " - Pivoting & Melting – Reshaping data for better analysis\n",
    " - Applying Functions (Apply, Map, Lambda) – Transforming data efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "160fca44-d610-4344-a0b7-40ee9b6f77b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category-wise Salary Stats:\n",
      "                   mean    max    min\n",
      "Category                            \n",
      "A         65750.000000  75000  50000\n",
      "B         58000.000000  62000  52000\n",
      "C         53333.333333  58000  48000\n",
      "\n",
      "Pivot Table (Category-wise Avg Salary):\n",
      "                 Salary\n",
      "Category              \n",
      "A         65750.000000\n",
      "B         58000.000000\n",
      "C         53333.333333\n",
      "\n",
      "✅ Data Transformation Completed! Saved as 'transformed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# 1️⃣ Handling Missing Values\n",
    "df.fillna({\"Age\": df[\"Age\"].median(), \"Salary\": df[\"Salary\"].mean()}, inplace=True)\n",
    "\n",
    "# 2️⃣ Scaling & Normalization\n",
    "scaler = MinMaxScaler()  # Normalization (0 to 1)\n",
    "if \"Salary\" in df.columns:\n",
    "    df[\"Salary_Scaled\"] = scaler.fit_transform(df[[\"Salary\"]])\n",
    "\n",
    "std_scaler = StandardScaler()  # Standardization (mean=0, std=1)\n",
    "if \"Age\" in df.columns:\n",
    "    df[\"Age_Standardized\"] = std_scaler.fit_transform(df[[\"Age\"]])\n",
    "\n",
    "# 3️⃣ Encoding Categorical Variables\n",
    "if \"Category\" in df.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"Category_Encoded\"] = label_encoder.fit_transform(df[\"Category\"].astype(str))\n",
    "\n",
    "# 4️⃣ Feature Engineering (Create New Features)\n",
    "if \"Salary\" in df.columns and \"Age\" in df.columns:\n",
    "    df[\"Salary_per_Age\"] = df[\"Salary\"] / df[\"Age\"]  # New Feature\n",
    "\n",
    "# 5️⃣ Aggregation & Grouping\n",
    "if \"Category\" in df.columns and \"Salary\" in df.columns:\n",
    "    category_summary = df.groupby(\"Category\")[\"Salary\"].agg([\"mean\", \"max\", \"min\"])\n",
    "    print(\"\\nCategory-wise Salary Stats:\\n\", category_summary)\n",
    "\n",
    "# 6️⃣ Pivoting & Melting (Reshaping Data)\n",
    "if \"Category\" in df.columns and \"Salary\" in df.columns:\n",
    "    pivot_df = df.pivot_table(values=\"Salary\", index=\"Category\", aggfunc=\"mean\")\n",
    "    print(\"\\nPivot Table (Category-wise Avg Salary):\\n\", pivot_df)\n",
    "\n",
    "# 7️⃣ Applying Functions (Apply, Map, Lambda)\n",
    "df[\"Salary_Level\"] = df[\"Salary\"].apply(lambda x: \"High\" if x > 50000 else \"Low\")\n",
    "\n",
    "# Save transformed data\n",
    "df.to_csv(\"transformed_data.csv\", index=False)\n",
    "print(\"\\n✅ Data Transformation Completed! Saved as 'transformed_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f81d0-7dac-4522-9a1f-c31d6350fa21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filtering and Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc40ec9-5fe5-48a9-b51d-ddc74c6e7bc6",
   "metadata": {},
   "source": [
    "Filtering and sorting are essential operations for data analysis, allowing you to extract meaningful insights from large datasets.\n",
    "\n",
    " - Filtering Rows: Extracting data based on conditions\n",
    " - Filtering with Multiple Conditions: Using & (AND) and | (OR) operators\n",
    " - Filtering with isin(), between(), and str.contains()\n",
    " - Sorting Data: Sorting rows based on column values (ascending/descending)\n",
    " - Sorting by Multiple Columns: Sorting using multiple criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064199cb-1f08-4351-934e-0adbc6d6aa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Employees with Salary > 50,000:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "1   2      Bob   30   60000        B   2019-06-20\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "4   5      Eva   22   52000        B   2020-09-30\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "7   8    Helen   29   62000        B   2021-10-22\n",
      "8   9      Ian   27   54000        C   2020-11-05\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "\n",
      "Employees older than 30 with Salary > 40,000:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "\n",
      "Employees from IT or Finance:\n",
      " Empty DataFrame\n",
      "Columns: [ID, Name, Age, Salary, Category, Joining_Date]\n",
      "Index: []\n",
      "\n",
      "Employees aged between 25 and 40:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "0   1    Alice   25   50000        A   2020-01-15\n",
      "1   2      Bob   30   60000        B   2019-06-20\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "3   4    David   28   48000        C   2021-07-25\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "7   8    Helen   29   62000        B   2021-10-22\n",
      "8   9      Ian   27   54000        C   2020-11-05\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "\n",
      "Employees with 'John' in Name:\n",
      " Empty DataFrame\n",
      "Columns: [ID, Name, Age, Salary, Category, Joining_Date]\n",
      "Index: []\n",
      "\n",
      "Employees Sorted by Salary (Descending Order):\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "7   8    Helen   29   62000        B   2021-10-22\n",
      "1   2      Bob   30   60000        B   2019-06-20\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "8   9      Ian   27   54000        C   2020-11-05\n",
      "4   5      Eva   22   52000        B   2020-09-30\n",
      "0   1    Alice   25   50000        A   2020-01-15\n",
      "3   4    David   28   48000        C   2021-07-25\n",
      "\n",
      "Employees Sorted by Category & Salary:\n",
      "    ID     Name  Age  Salary Category Joining_Date\n",
      "5   6    Frank   40   75000        A   2017-12-11\n",
      "2   3  Charlie   35   70000        A   2018-03-12\n",
      "9  10     Jack   36   68000        A   2018-08-17\n",
      "0   1    Alice   25   50000        A   2020-01-15\n",
      "7   8    Helen   29   62000        B   2021-10-22\n",
      "1   2      Bob   30   60000        B   2019-06-20\n",
      "4   5      Eva   22   52000        B   2020-09-30\n",
      "6   7    Grace   32   58000        C   2019-05-18\n",
      "8   9      Ian   27   54000        C   2020-11-05\n",
      "3   4    David   28   48000        C   2021-07-25\n",
      "\n",
      "✅ Filtering and Sorting Completed! Saved as 'sorted_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# 1️⃣ Filtering Rows: Select employees with Salary > 50,000\n",
    "high_salary = df[df[\"Salary\"] > 50000]\n",
    "print(\"\\nEmployees with Salary > 50,000:\\n\", high_salary)\n",
    "\n",
    "# 2️⃣ Filtering with Multiple Conditions: Age > 30 and Salary > 40,000\n",
    "filtered_data = df[(df[\"Age\"] > 30) & (df[\"Salary\"] > 40000)]\n",
    "print(\"\\nEmployees older than 30 with Salary > 40,000:\\n\", filtered_data)\n",
    "\n",
    "# 3️⃣ Filtering with isin(): Select employees from a specific category\n",
    "if \"Category\" in df.columns:\n",
    "    selected_categories = df[df[\"Category\"].isin([\"IT\", \"Finance\"])]\n",
    "    print(\"\\nEmployees from IT or Finance:\\n\", selected_categories)\n",
    "\n",
    "# 4️⃣ Filtering with between(): Select employees with Age between 25 and 40\n",
    "age_filtered = df[df[\"Age\"].between(25, 40)]\n",
    "print(\"\\nEmployees aged between 25 and 40:\\n\", age_filtered)\n",
    "\n",
    "# 5️⃣ Filtering with str.contains(): Select names containing \"John\"\n",
    "if \"Name\" in df.columns:\n",
    "    name_filter = df[df[\"Name\"].str.contains(\"John\", case=False, na=False)]\n",
    "    print(\"\\nEmployees with 'John' in Name:\\n\", name_filter)\n",
    "\n",
    "# 6️⃣ Sorting Data: Sorting by Salary in Descending Order\n",
    "sorted_salary = df.sort_values(by=\"Salary\", ascending=False)\n",
    "print(\"\\nEmployees Sorted by Salary (Descending Order):\\n\", sorted_salary)\n",
    "\n",
    "# 7️⃣ Sorting by Multiple Columns: First by Category, then by Salary (Descending)\n",
    "if \"Category\" in df.columns:\n",
    "    sorted_multi = df.sort_values(by=[\"Category\", \"Salary\"], ascending=[True, False])\n",
    "    print(\"\\nEmployees Sorted by Category & Salary:\\n\", sorted_multi)\n",
    "\n",
    "# Save sorted data\n",
    "sorted_salary.to_csv(\"sorted_data.csv\", index=False)\n",
    "print(\"\\n✅ Filtering and Sorting Completed! Saved as 'sorted_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25d0a3-d8ba-447b-b98f-5a038ea388b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  Grouping and Aggregating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f9f7b-5b5c-4b94-b0eb-4b2404b51a2f",
   "metadata": {},
   "source": [
    "Grouping and aggregating allow you to summarize data by categories, which is useful for data analysis and machine learning preprocessing.\n",
    "\n",
    " - Grouping Data: Using groupby() to group data by a column.\n",
    " - Aggregating Data: Using functions like sum(), mean(), count(), min(), max().\n",
    " - Multiple Aggregations: Applying multiple aggregation functions at once.\n",
    " - Grouping Multiple Columns: Grouping based on multiple categories.\n",
    " - Custom Aggregations: Using agg() for flexible operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb8461e9-4838-4ff0-8b31-07525eb1b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Employees in Each Category:\n",
      " Category\n",
      "A    4\n",
      "B    3\n",
      "C    3\n",
      "Name: Name, dtype: int64\n",
      "\n",
      "Average Salary per Category:\n",
      " Category\n",
      "A    65750.000000\n",
      "B    58000.000000\n",
      "C    53333.333333\n",
      "Name: Salary, dtype: float64\n",
      "\n",
      "Salary Statistics per Category:\n",
      "              sum          mean    max\n",
      "Category                             \n",
      "A         263000  65750.000000  75000\n",
      "B         174000  58000.000000  62000\n",
      "C         160000  53333.333333  58000\n",
      "\n",
      "Number of Employees in Each Category and Age Group:\n",
      " Category  Age\n",
      "A         25     1\n",
      "          35     1\n",
      "          36     1\n",
      "          40     1\n",
      "B         22     1\n",
      "          29     1\n",
      "          30     1\n",
      "C         27     1\n",
      "          28     1\n",
      "          32     1\n",
      "Name: Name, dtype: int64\n",
      "\n",
      "Custom Aggregation for Salary and Age:\n",
      "         Salary   Age\n",
      "mean   59700.0  30.4\n",
      "sum   597000.0   NaN\n",
      "max    75000.0  40.0\n",
      "min        NaN  22.0\n",
      "\n",
      "✅ Grouping and Aggregation Completed! Saved as 'aggregated_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# 1️⃣ Grouping by a Single Column (Category) and Counting Entries\n",
    "if \"Category\" in df.columns:\n",
    "    category_counts = df.groupby(\"Category\")[\"Name\"].count()\n",
    "    print(\"\\nNumber of Employees in Each Category:\\n\", category_counts)\n",
    "\n",
    "# 2️⃣ Aggregating Data: Mean Salary per Category\n",
    "if \"Category\" in df.columns and \"Salary\" in df.columns:\n",
    "    avg_salary = df.groupby(\"Category\")[\"Salary\"].mean()\n",
    "    print(\"\\nAverage Salary per Category:\\n\", avg_salary)\n",
    "\n",
    "# 3️⃣ Applying Multiple Aggregations: Sum, Mean, and Max of Salary per Category\n",
    "if \"Category\" in df.columns and \"Salary\" in df.columns:\n",
    "    salary_stats = df.groupby(\"Category\")[\"Salary\"].agg([\"sum\", \"mean\", \"max\"])\n",
    "    print(\"\\nSalary Statistics per Category:\\n\", salary_stats)\n",
    "\n",
    "# 4️⃣ Grouping by Multiple Columns: Count Employees per Category and Age Group\n",
    "if \"Category\" in df.columns and \"Age\" in df.columns:\n",
    "    multi_group = df.groupby([\"Category\", \"Age\"])[\"Name\"].count()\n",
    "    print(\"\\nNumber of Employees in Each Category and Age Group:\\n\", multi_group)\n",
    "\n",
    "# 5️⃣ Custom Aggregation Using agg(): Aggregating Salary and Age\n",
    "if \"Salary\" in df.columns and \"Age\" in df.columns:\n",
    "    custom_agg = df.agg({\"Salary\": [\"mean\", \"sum\", \"max\"], \"Age\": [\"mean\", \"min\", \"max\"]})\n",
    "    print(\"\\nCustom Aggregation for Salary and Age:\\n\", custom_agg)\n",
    "\n",
    "# Save aggregated data\n",
    "salary_stats.to_csv(\"aggregated_data.csv\")\n",
    "print(\"\\n✅ Grouping and Aggregation Completed! Saved as 'aggregated_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d649c-c500-48c0-9b8e-ac648397bf98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Merging and Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94321d20-f9b4-4962-86a9-57d14726321a",
   "metadata": {},
   "source": [
    "Merging and joining allow you to combine multiple datasets efficiently, which is crucial for data preprocessing in machine learning.\n",
    "\n",
    " - Merging DataFrames: Using merge() to combine datasets based on common columns.\n",
    " - Types of Joins: Inner, Left, Right, and Outer Joins.\n",
    " - Joining on Index: Using set_index() and join().\n",
    " - Concatenating DataFrames: Using concat() for stacking data.\n",
    " - Handling Duplicates & Conflicts: Managing merge conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d874c1de-a41d-4f44-8e11-bf5b3201069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 DataFrame 1:\n",
      "    ID     Name  Age\n",
      "0   1    Alice   25\n",
      "1   2      Bob   30\n",
      "2   3  Charlie   35\n",
      "3   4    David   40\n",
      "\n",
      "🔹 DataFrame 2:\n",
      "    ID  Salary Department\n",
      "0   3   60000         IT\n",
      "1   4   70000         HR\n",
      "2   5   80000    Finance\n",
      "3   6   90000  Marketing\n",
      "\n",
      "✅ Inner Join Result:\n",
      "    ID     Name  Age  Salary Department\n",
      "0   3  Charlie   35   60000         IT\n",
      "1   4    David   40   70000         HR\n",
      "\n",
      "✅ Left Join Result:\n",
      "    ID     Name  Age   Salary Department\n",
      "0   1    Alice   25      NaN        NaN\n",
      "1   2      Bob   30      NaN        NaN\n",
      "2   3  Charlie   35  60000.0         IT\n",
      "3   4    David   40  70000.0         HR\n",
      "\n",
      "✅ Right Join Result:\n",
      "    ID     Name   Age  Salary Department\n",
      "0   3  Charlie  35.0   60000         IT\n",
      "1   4    David  40.0   70000         HR\n",
      "2   5      NaN   NaN   80000    Finance\n",
      "3   6      NaN   NaN   90000  Marketing\n",
      "\n",
      "✅ Outer Join Result:\n",
      "    ID     Name   Age   Salary Department\n",
      "0   1    Alice  25.0      NaN        NaN\n",
      "1   2      Bob  30.0      NaN        NaN\n",
      "2   3  Charlie  35.0  60000.0         IT\n",
      "3   4    David  40.0  70000.0         HR\n",
      "4   5      NaN   NaN  80000.0    Finance\n",
      "5   6      NaN   NaN  90000.0  Marketing\n",
      "\n",
      "✅ Join on Index Result:\n",
      "        Name   Age   Salary Department\n",
      "ID                                   \n",
      "1     Alice  25.0      NaN        NaN\n",
      "2       Bob  30.0      NaN        NaN\n",
      "3   Charlie  35.0  60000.0         IT\n",
      "4     David  40.0  70000.0         HR\n",
      "5       NaN   NaN  80000.0    Finance\n",
      "6       NaN   NaN  90000.0  Marketing\n",
      "\n",
      "✅ Concatenated DataFrame:\n",
      "    ID     Name   Age   Salary Department\n",
      "0   1    Alice  25.0      NaN        NaN\n",
      "1   2      Bob  30.0      NaN        NaN\n",
      "2   3  Charlie  35.0      NaN        NaN\n",
      "3   4    David  40.0      NaN        NaN\n",
      "4   3      NaN   NaN  60000.0         IT\n",
      "5   4      NaN   NaN  70000.0         HR\n",
      "6   5      NaN   NaN  80000.0    Finance\n",
      "7   6      NaN   NaN  90000.0  Marketing\n",
      "\n",
      "📂 Merged Data Saved as 'merged_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating Sample DataFrames\n",
    "data1 = {\n",
    "    \"ID\": [1, 2, 3, 4],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"Age\": [25, 30, 35, 40]\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {\n",
    "    \"ID\": [3, 4, 5, 6],\n",
    "    \"Salary\": [60000, 70000, 80000, 90000],\n",
    "    \"Department\": [\"IT\", \"HR\", \"Finance\", \"Marketing\"]\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "print(\"🔹 DataFrame 1:\\n\", df1)\n",
    "print(\"\\n🔹 DataFrame 2:\\n\", df2)\n",
    "\n",
    "# 1️⃣ **Inner Join (Default) - Keeps only common IDs**\n",
    "merged_inner = pd.merge(df1, df2, on=\"ID\", how=\"inner\")\n",
    "print(\"\\n✅ Inner Join Result:\\n\", merged_inner)\n",
    "\n",
    "# 2️⃣ **Left Join - Keeps all rows from df1, matching with df2**\n",
    "merged_left = pd.merge(df1, df2, on=\"ID\", how=\"left\")\n",
    "print(\"\\n✅ Left Join Result:\\n\", merged_left)\n",
    "\n",
    "# 3️⃣ **Right Join - Keeps all rows from df2, matching with df1**\n",
    "merged_right = pd.merge(df1, df2, on=\"ID\", how=\"right\")\n",
    "print(\"\\n✅ Right Join Result:\\n\", merged_right)\n",
    "\n",
    "# 4️⃣ **Outer Join - Keeps all data from both DataFrames**\n",
    "merged_outer = pd.merge(df1, df2, on=\"ID\", how=\"outer\")\n",
    "print(\"\\n✅ Outer Join Result:\\n\", merged_outer)\n",
    "\n",
    "# 5️⃣ **Joining on Index**\n",
    "df1.set_index(\"ID\", inplace=True)\n",
    "df2.set_index(\"ID\", inplace=True)\n",
    "joined_df = df1.join(df2, how=\"outer\")\n",
    "print(\"\\n✅ Join on Index Result:\\n\", joined_df)\n",
    "\n",
    "# 6️⃣ **Concatenation (Vertical Stacking)**\n",
    "df_concat = pd.concat([df1.reset_index(), df2.reset_index()], axis=0, ignore_index=True)\n",
    "print(\"\\n✅ Concatenated DataFrame:\\n\", df_concat)\n",
    "\n",
    "# Save the merged DataFrame\n",
    "merged_outer.to_csv(\"merged_data.csv\", index=False)\n",
    "print(\"\\n📂 Merged Data Saved as 'merged_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66022ba3-ecf9-432b-910f-f0b8c1af6394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Handling Dates and Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16193f09-4747-4e71-a74b-672f559fd02e",
   "metadata": {},
   "source": [
    "Date and time handling is crucial in data analysis, especially for time-series data, scheduling, and trend analysis.\n",
    "\n",
    " - Parsing Dates from Strings (pd.to_datetime())\n",
    " - Extracting Components (Year, Month, Day, etc.)\n",
    " - Date Arithmetic (Adding/Subtracting Days)\n",
    " - Handling Missing Dates\n",
    " - Working with Time Deltas\n",
    " - Resampling Time-Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48a4d7a7-73d9-4bbf-89c7-b13e1f0563d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Resampled Monthly Data Count:\n",
      "             Event  Year  Month  Day  Weekday  Next Day  Previous Week  \\\n",
      "Date                                                                    \n",
      "2025-03-31      2     2      2    2        2         2              2   \n",
      "2025-04-30      2     2      2    2        2         2              2   \n",
      "\n",
      "            Days Until Event  \n",
      "Date                          \n",
      "2025-03-31                 2  \n",
      "2025-04-30                 2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1080\\4009223867.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df.loc[4] = [\"Hackathon\", pd.NaT, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"Event\": [\"Meeting\", \"Project Deadline\", \"Conference\", \"Workshop\"],\n",
    "    \"Date\": [\"2025-03-01\", \"2025-03-15\", \"2025-04-10\", \"2025-04-20\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'Date' column to DateTime format\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Extract Date Components\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "df[\"Month\"] = df[\"Date\"].dt.month\n",
    "df[\"Day\"] = df[\"Date\"].dt.day\n",
    "df[\"Weekday\"] = df[\"Date\"].dt.day_name()\n",
    "\n",
    "# Date Arithmetic\n",
    "df[\"Next Day\"] = df[\"Date\"] + pd.Timedelta(days=1)\n",
    "df[\"Previous Week\"] = df[\"Date\"] - pd.Timedelta(weeks=1)\n",
    "\n",
    "# Handling Missing Dates Properly\n",
    "df.loc[4] = [\"Hackathon\", pd.NaT, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA]\n",
    "df = df.convert_dtypes()  # Convert dtypes properly\n",
    "\n",
    "# Time Delta Calculation\n",
    "df[\"Days Until Event\"] = (df[\"Date\"] - pd.to_datetime(\"2025-03-01\")).dt.days\n",
    "\n",
    "# Resampling with Updated Method\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "print(\"\\n✅ Resampled Monthly Data Count:\\n\", df.resample(\"ME\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c665e7c-0cc2-4e53-98c6-17e135859ac7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81a0a7-bd12-4cee-a278-f61b6207190c",
   "metadata": {},
   "source": [
    "Pivot tables in Pandas are used to summarize and analyze data in a structured manner. They allow us to:\n",
    "\n",
    " - Group data by a particular column\n",
    " - Aggregate values using functions like sum(), mean(), count()\n",
    " - Reshape data efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8de2b8c6-5ad8-4b7e-aa2b-cfbb7f1e94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Average Salary by Department:\n",
      "              Salary\n",
      "Department         \n",
      "Finance     63500.0\n",
      "HR          52500.0\n",
      "IT          70000.0\n",
      "\n",
      "✅ Total Salary and Experience per Department:\n",
      "             Experience  Salary\n",
      "Department                    \n",
      "Finance             14  127000\n",
      "HR                  12  105000\n",
      "IT                  17  210000\n",
      "\n",
      "✅ Salary Breakdown by Department and Employee:\n",
      "                      Salary\n",
      "Department Employee        \n",
      "Finance    Frank      65000\n",
      "           Grace      62000\n",
      "HR         Alice      50000\n",
      "           Bob        55000\n",
      "IT         Charlie    70000\n",
      "           David      72000\n",
      "           Eve        68000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 📌 Sample Data\n",
    "data = {\n",
    "    \"Department\": [\"HR\", \"HR\", \"IT\", \"IT\", \"IT\", \"Finance\", \"Finance\"],\n",
    "    \"Employee\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\"],\n",
    "    \"Salary\": [50000, 55000, 70000, 72000, 68000, 65000, 62000],\n",
    "    \"Experience\": [5, 7, 3, 8, 6, 10, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 🔥 Pivot Table: Average Salary per Department\n",
    "pivot_table = df.pivot_table(values=\"Salary\", index=\"Department\", aggfunc=\"mean\")\n",
    "\n",
    "print(\"\\n✅ Average Salary by Department:\\n\", pivot_table)\n",
    "\n",
    "# 🔥 Pivot Table: Sum of Salaries with Experience\n",
    "pivot_table2 = df.pivot_table(values=[\"Salary\", \"Experience\"], index=\"Department\", aggfunc=\"sum\")\n",
    "\n",
    "print(\"\\n✅ Total Salary and Experience per Department:\\n\", pivot_table2)\n",
    "\n",
    "# 🔥 Pivot Table: Multi-Level Index (Grouping by Department and Employee)\n",
    "pivot_table3 = df.pivot_table(values=\"Salary\", index=[\"Department\", \"Employee\"], aggfunc=\"sum\")\n",
    "\n",
    "print(\"\\n✅ Salary Breakdown by Department and Employee:\\n\", pivot_table3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d107e1-a854-4e3f-a347-62108a7f1b96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Working with Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018b7ef-0478-49ee-8e59-b4e74369ec9d",
   "metadata": {},
   "source": [
    "Use techniques like chunking to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c07b2c7-463b-40c9-83c9-9743d92e3bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2025.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (23.2)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Collecting locket (from partd>=1.4.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading dask-2025.3.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.4 MB 187.9 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.0/1.4 MB 196.9 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/1.4 MB 252.2 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.4 MB 252.2 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.4 MB 252.2 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.4 MB 252.2 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.4 MB 252.2 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.4 MB 252.2 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.4 MB 126.9 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.4 MB 126.9 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.4 MB 126.9 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.4 MB 126.9 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.4 MB 126.9 kB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.1/1.4 MB 111.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.1/1.4 MB 111.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.1/1.4 MB 111.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.1/1.4 MB 111.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.1/1.4 MB 111.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.1/1.4 MB 111.6 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.1/1.4 MB 102.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.1/1.4 MB 102.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.1/1.4 MB 102.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.1/1.4 MB 102.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.1/1.4 MB 93.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 93.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 93.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 93.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 93.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 93.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.1/1.4 MB 91.6 kB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 76.5 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 74.4 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 74.4 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 74.4 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 0.2/1.4 MB 74.4 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 77.1 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 77.1 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 77.1 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 77.1 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 75.0 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 75.0 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 75.0 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 0.2/1.4 MB 78.2 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 78.2 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 78.2 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 78.2 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 78.2 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 75.9 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 75.9 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 75.9 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 75.9 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 75.9 kB/s eta 0:00:16\n",
      "   ------ --------------------------------- 0.2/1.4 MB 75.9 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 0.3/1.4 MB 76.0 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 0.3/1.4 MB 76.0 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 0.3/1.4 MB 76.0 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 0.3/1.4 MB 76.0 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 0.3/1.4 MB 77.5 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 0.3/1.4 MB 77.5 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 0.3/1.4 MB 77.5 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 0.3/1.4 MB 77.5 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 0.3/1.4 MB 76.3 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 0.3/1.4 MB 76.3 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.9 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.9 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.9 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.9 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.0 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.0 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.3/1.4 MB 79.0 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 0.3/1.4 MB 81.6 kB/s eta 0:00:14\n",
      "   --------- ------------------------------ 0.3/1.4 MB 81.6 kB/s eta 0:00:14\n",
      "   --------- ------------------------------ 0.4/1.4 MB 84.7 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.4/1.4 MB 84.7 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.4/1.4 MB 84.7 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 0.4/1.4 MB 84.3 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 0.4/1.4 MB 84.3 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 0.4/1.4 MB 84.3 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 0.4/1.4 MB 86.3 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 0.4/1.4 MB 86.3 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 86.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 83.0 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 83.0 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 83.0 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 83.0 kB/s eta 0:00:13\n",
      "   ------------ --------------------------- 0.4/1.4 MB 83.9 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.4/1.4 MB 83.9 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.4/1.4 MB 83.9 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.4/1.4 MB 83.9 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.5/1.4 MB 83.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.5/1.4 MB 83.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.5/1.4 MB 83.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.5/1.4 MB 83.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.5/1.4 MB 83.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 0.5/1.4 MB 83.1 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 82.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 82.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 82.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 82.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 82.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 81.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 81.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 81.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 81.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 81.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 81.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 80.9 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 80.9 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 80.9 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 80.9 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.4 MB 80.9 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 81.1 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 81.1 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 81.1 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 81.1 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 80.2 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 80.2 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 80.2 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 80.2 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 0.5/1.4 MB 80.2 kB/s eta 0:00:12\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.6 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.6 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.6 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.6 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 0.6/1.4 MB 80.1 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 79.8 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 79.8 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 79.8 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 79.8 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 80.2 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 80.2 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 80.2 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 80.2 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 80.2 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.4 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.4 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.4 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.4 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.4 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.6 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.6 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.6 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.6 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 79.1 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 0.7/1.4 MB 78.0 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 77.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.5 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.5 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.5 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 0.7/1.4 MB 75.5 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 75.3 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 76.6 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 76.6 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 76.6 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 76.6 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 0.7/1.4 MB 76.6 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.9 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.9 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.9 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.9 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.9 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 0.8/1.4 MB 76.0 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.5 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.5 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 74.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 72.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 71.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 0.9/1.4 MB 70.8 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.2 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.2 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.2 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.3 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.3 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.3 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 0.9/1.4 MB 70.3 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 70.9 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 69.0 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 69.0 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 69.0 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 69.0 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 0.9/1.4 MB 69.0 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 0.9/1.4 MB 68.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 1.0/1.4 MB 68.0 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.0/1.4 MB 68.0 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.0/1.4 MB 68.0 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.0/1.4 MB 68.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 67.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.0/1.4 MB 64.6 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 64.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 63.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 63.6 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 62.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.1/1.4 MB 59.2 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 59.3 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 58.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 58.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.1/1.4 MB 58.1 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 1.2/1.4 MB 58.7 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 1.2/1.4 MB 58.7 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 1.2/1.4 MB 58.7 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 1.2/1.4 MB 58.8 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 1.2/1.4 MB 58.8 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 1.2/1.4 MB 58.8 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 1.2/1.4 MB 59.5 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 1.2/1.4 MB 59.5 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 1.2/1.4 MB 59.5 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 1.2/1.4 MB 59.5 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.2/1.4 MB 60.1 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 60.5 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 60.5 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 60.8 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 60.8 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 60.8 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 61.2 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 1.3/1.4 MB 61.7 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.3/1.4 MB 62.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 62.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.4/1.4 MB 61.5 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 61.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 59.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 57.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 57.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 57.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 57.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 57.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 58.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 58.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 58.1 kB/s eta 0:00:00\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: locket, partd, dask\n",
      "Successfully installed dask-2025.3.0 locket-1.0.0 partd-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6df559e2-8c50-4353-b847-e20d1fc7ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Employee_ID            Name Department  Salary  Experience  \\\n",
      "0            1    Bethany Mack      Sales  114401          31   \n",
      "1            2    Dakota Clark         HR   33333          16   \n",
      "2            3  Curtis Mendoza  Marketing   52970          36   \n",
      "3            4     Mary Gibson         IT  126713          19   \n",
      "4            5    Joseph Green    Finance   74858          39   \n",
      "\n",
      "       Date_of_Joining  \n",
      "0  2020-09-29 04:25:54  \n",
      "1  2022-10-20 04:44:41  \n",
      "2  2022-06-19 06:23:47  \n",
      "3  2020-12-10 19:32:05  \n",
      "4  2022-09-26 14:17:31  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   Employee_ID      100000 non-null  int32         \n",
      " 1   Name             100000 non-null  string        \n",
      " 2   Department       100000 non-null  category      \n",
      " 3   Salary           100000 non-null  int32         \n",
      " 4   Experience       100000 non-null  int8          \n",
      " 5   Date_of_Joining  100000 non-null  datetime64[ns]\n",
      "dtypes: category(1), datetime64[ns](1), int32(2), int8(1), string(1)\n",
      "memory usage: 2.5 MB\n",
      "\n",
      "✅ Optimized Memory Usage:\n",
      " None\n",
      "\n",
      "✅ Limited Data Sample:\n",
      "    Employee_ID  Salary\n",
      "0            1  114401\n",
      "1            2   33333\n",
      "2            3   52970\n",
      "3            4  126713\n",
      "4            5   74858\n",
      "\n",
      "✅ Dask Data Processing:\n",
      "    Employee_ID            Name Department  Salary  Experience  \\\n",
      "0            1    Bethany Mack      Sales  114401          31   \n",
      "1            2    Dakota Clark         HR   33333          16   \n",
      "2            3  Curtis Mendoza  Marketing   52970          36   \n",
      "3            4     Mary Gibson         IT  126713          19   \n",
      "4            5    Joseph Green    Finance   74858          39   \n",
      "\n",
      "      Date_of_Joining  \n",
      "0 2020-09-29 04:25:54  \n",
      "1 2022-10-20 04:44:41  \n",
      "2 2022-06-19 06:23:47  \n",
      "3 2020-12-10 19:32:05  \n",
      "4 2022-09-26 14:17:31  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd  # For large-scale data handling\n",
    "\n",
    "file_path = \"small_dataset.csv\"  # Update with your actual dataset path\n",
    "\n",
    "# 1️⃣ Reading Large CSV in Chunks (Memory Efficient)\n",
    "chunk_size = 10_000  # Load 10K rows at a time\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in chunks:\n",
    "    print(chunk.head())  # Process data here (e.g., filtering, aggregation)\n",
    "    break  # Remove this to process full file\n",
    "\n",
    "# 2️⃣ Optimizing Memory Usage with `dtype`\n",
    "dtype_dict = {\n",
    "    \"Employee_ID\": \"int32\",\n",
    "    \"Name\": \"string\",\n",
    "    \"Department\": \"category\",  # Uses less memory\n",
    "    \"Salary\": \"int32\",\n",
    "    \"Experience\": \"int8\"\n",
    "}\n",
    "\n",
    "df_optimized = pd.read_csv(file_path, dtype=dtype_dict, parse_dates=[\"Date_of_Joining\"])\n",
    "print(\"\\n✅ Optimized Memory Usage:\\n\", df_optimized.info())\n",
    "\n",
    "# 3️⃣ Using `nrows` and `usecols` to Load Selective Data\n",
    "df_limited = pd.read_csv(file_path, nrows=5000, usecols=[\"Employee_ID\", \"Salary\"])\n",
    "print(\"\\n✅ Limited Data Sample:\\n\", df_limited.head())\n",
    "\n",
    "# 4️⃣ Using Dask for Very Large Data Handling\n",
    "df_dask = dd.read_csv(file_path, dtype=dtype_dict, parse_dates=[\"Date_of_Joining\"])\n",
    "print(\"\\n✅ Dask Data Processing:\\n\", df_dask.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925085d9-38b2-43f5-b2aa-e071ae16ca53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Advanced Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17583fd9-8d4f-46ec-9329-28691cd57af9",
   "metadata": {},
   "source": [
    " - Pivot & Melt → Convert data between long and wide formats.\n",
    " - apply() & map() → Apply functions to transform data efficiently.\n",
    " - Multi-Indexing → Handle hierarchical data structure.\n",
    " - Rolling Window → Perform moving average calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "996d62f3-2863-46c8-a519-396740c089a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Original DataFrame:\n",
      "   Category Type  Value1  Value2\n",
      "0        A    X      10       5\n",
      "1        A    Y      20      10\n",
      "2        B    X      15       7\n",
      "3        B    Y      25      12\n",
      "4        C    X      30      15\n",
      "5        C    Y      35      18\n",
      "\n",
      "🔹 Pivot Table:\n",
      " Type       X   Y\n",
      "Category        \n",
      "A         10  20\n",
      "B         15  25\n",
      "C         30  35\n",
      "\n",
      "🔹 Melted DataFrame:\n",
      "    Category Type  Metric  Amount\n",
      "0         A    X  Value1      10\n",
      "1         A    Y  Value1      20\n",
      "2         B    X  Value1      15\n",
      "3         B    Y  Value1      25\n",
      "4         C    X  Value1      30\n",
      "5         C    Y  Value1      35\n",
      "6         A    X  Value2       5\n",
      "7         A    Y  Value2      10\n",
      "8         B    X  Value2       7\n",
      "9         B    Y  Value2      12\n",
      "10        C    X  Value2      15\n",
      "11        C    Y  Value2      18\n",
      "\n",
      "🔹 Applying Function to Square Value1:\n",
      "   Category Type  Value1  Value2  Value1_Squared\n",
      "0        A    X      10       5             100\n",
      "1        A    Y      20      10             400\n",
      "2        B    X      15       7             225\n",
      "3        B    Y      25      12             625\n",
      "4        C    X      30      15             900\n",
      "5        C    Y      35      18            1225\n",
      "\n",
      "🔹 Mapped Category to Numerical Codes:\n",
      "   Category Type  Value1  Value2  Value1_Squared  Category_Code\n",
      "0        A    X      10       5             100              1\n",
      "1        A    Y      20      10             400              1\n",
      "2        B    X      15       7             225              2\n",
      "3        B    Y      25      12             625              2\n",
      "4        C    X      30      15             900              3\n",
      "5        C    Y      35      18            1225              3\n",
      "\n",
      "🔹 Multi-Indexed DataFrame:\n",
      "                Value1  Value2  Value1_Squared  Category_Code\n",
      "Category Type                                               \n",
      "A        X         10       5             100              1\n",
      "         Y         20      10             400              1\n",
      "B        X         15       7             225              2\n",
      "         Y         25      12             625              2\n",
      "C        X         30      15             900              3\n",
      "         Y         35      18            1225              3\n",
      "\n",
      "🔹 Rolling Mean (Window=2) on Value1:\n",
      "   Category Type  Value1  Value2  Value1_Squared  Category_Code  Rolling_Mean\n",
      "0        A    X      10       5             100              1           NaN\n",
      "1        A    Y      20      10             400              1          15.0\n",
      "2        B    X      15       7             225              2          17.5\n",
      "3        B    Y      25      12             625              2          20.0\n",
      "4        C    X      30      15             900              3          27.5\n",
      "5        C    Y      35      18            1225              3          32.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    \"Category\": [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"],\n",
    "    \"Type\": [\"X\", \"Y\", \"X\", \"Y\", \"X\", \"Y\"],\n",
    "    \"Value1\": [10, 20, 15, 25, 30, 35],\n",
    "    \"Value2\": [5, 10, 7, 12, 15, 18],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"🔹 Original DataFrame:\\n\", df)\n",
    "\n",
    "# 🔹 1️⃣ Reshaping with Pivot\n",
    "pivot_df = df.pivot(index=\"Category\", columns=\"Type\", values=\"Value1\")\n",
    "print(\"\\n🔹 Pivot Table:\\n\", pivot_df)\n",
    "\n",
    "# 🔹 2️⃣ Melting (Reverse of Pivot)\n",
    "melted_df = df.melt(id_vars=[\"Category\", \"Type\"], var_name=\"Metric\", value_name=\"Amount\")\n",
    "print(\"\\n🔹 Melted DataFrame:\\n\", melted_df)\n",
    "\n",
    "# 🔹 3️⃣ Applying Functions (Transform)\n",
    "df[\"Value1_Squared\"] = df[\"Value1\"].apply(lambda x: x**2)\n",
    "print(\"\\n🔹 Applying Function to Square Value1:\\n\", df)\n",
    "\n",
    "# 🔹 4️⃣ Using map() for Element-wise Mapping\n",
    "df[\"Category_Code\"] = df[\"Category\"].map({\"A\": 1, \"B\": 2, \"C\": 3})\n",
    "print(\"\\n🔹 Mapped Category to Numerical Codes:\\n\", df)\n",
    "\n",
    "# 🔹 5️⃣ Multi-Indexing\n",
    "multi_index_df = df.set_index([\"Category\", \"Type\"])\n",
    "print(\"\\n🔹 Multi-Indexed DataFrame:\\n\", multi_index_df)\n",
    "\n",
    "# 🔹 6️⃣ Rolling Window Calculation (Moving Average)\n",
    "df[\"Rolling_Mean\"] = df[\"Value1\"].rolling(window=2).mean()\n",
    "print(\"\\n🔹 Rolling Mean (Window=2) on Value1:\\n\", df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
